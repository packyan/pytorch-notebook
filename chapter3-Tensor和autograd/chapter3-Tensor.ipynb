{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章 PyTorch基础：Tensor和Autograd\n",
    "\n",
    "## 3.1 Tensor\n",
    "\n",
    "Tensor，又名张量，读者可能对这个名词似曾相识，因它不仅在PyTorch中出现过，它也是Theano、TensorFlow、\n",
    "Torch和MxNet中重要的数据结构。关于张量的本质不乏深度的剖析，但从工程角度来讲，可简单地认为它就是一个数组，且支持高效的科学计算。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）和更高维的数组（高阶数据）。Tensor和Numpy的ndarrays类似，但PyTorch的tensor支持GPU加速。\n",
    "\n",
    "本节将系统讲解tensor的使用，力求面面俱到，但不会涉及每个函数。对于更多函数及其用法，读者可通过在IPython/Notebook中使用函数名加`?`查看帮助文档，或查阅PyTorch官方文档[^1]。\n",
    "\n",
    "[^1]: http://docs.pytorch.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's begin\n",
    "from __future__ import print_function\n",
    "import torch  as t\n",
    "t.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1.1 基础操作\n",
    "\n",
    "学习过Numpy的读者会对本节内容感到非常熟悉，因tensor的接口有意设计成与Numpy类似，以方便用户使用。但不熟悉Numpy也没关系，本节内容并不要求先掌握Numpy。\n",
    "\n",
    "从接口的角度来讲，对tensor的操作可分为两类：\n",
    "\n",
    "1. `torch.function`，如`torch.save`等。\n",
    "2. 另一类是`tensor.function`，如`tensor.view`等。\n",
    "\n",
    "为方便使用，对tensor的大部分操作同时支持这两类接口，在本书中不做具体区分，如`torch.sum (torch.sum(a, b))`与`tensor.sum (a.sum(b))`功能等价。\n",
    "\n",
    "而从存储的角度来讲，对tensor的操作又可分为两类：\n",
    "\n",
    "1. 不会修改自身的数据，如 `a.add(b)`， 加法的结果会返回一个新的tensor。\n",
    "2. 会修改自身的数据，如 `a.add_(b)`， 加法的结果仍存储在a中，a被修改了。\n",
    "\n",
    "函数名以`_`结尾的都是`inplace`方式, 即会修改调用者自己的数据，在实际应用中需加以区分。\n",
    "\n",
    "#### 创建Tensor\n",
    "\n",
    "在PyTorch中新建tensor的方法有很多，具体如表3-1所示。\n",
    "\n",
    "表3-1: 常见新建tensor的方法\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(\\*sizes)|基础构造函数|\n",
    "|tensor(data,)|类似np.array的构造函数|\n",
    "|ones(\\*sizes)|全1Tensor|\n",
    "|zeros(\\*sizes)|全0Tensor|\n",
    "|eye(\\*sizes)|对角线为1，其他为0|\n",
    "|arange(s,e,step|从s到e，步长为step|\n",
    "|linspace(s,e,steps)|从s到e，均匀切分成steps份|\n",
    "|rand/randn(\\*sizes)|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|\n",
    "\n",
    "这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu).\n",
    "\n",
    "\n",
    "其中使用`Tensor`函数新建tensor是最复杂多变的方式，它既可以接收一个list，并根据list的数据新建tensor，也能根据指定的形状新建tensor，还能传入其他的tensor，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定tensor的形状\n",
    "a = t.Tensor(2, 3)\n",
    "a # 数值取决于内存空间的状态，print时候可能overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用list的数据创建tensor\n",
    "b = t.Tensor([[1,2,3],[4,5,6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist() # 把tensor转为list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensor.size()`返回`torch.Size`对象，它是tuple的子类，但其使用方式与tuple略有区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numel() # b中元素总个数，2*3，等价于b.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000]]), tensor([2., 3.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个和b形状一样的tensor\n",
    "c = t.Tensor(b_size)\n",
    "# 创建一个元素为2和3的tensor\n",
    "d = t.Tensor((2, 3))\n",
    "c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了`tensor.size()`，还可以利用`tensor.shape`直接查看tensor的形状，`tensor.shape`等价于`tensor.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，`t.Tensor(*sizes)`创建tensor时，系统不会马上分配空间，只是会计算剩余的内存是否足够使用，使用到tensor时才会分配，而其它操作都是在创建完tensor之后马上进行空间分配。其它常用的创建tensor的方法举例如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4142, -0.2510,  0.7874],\n",
       "        [-1.3061, -0.3556, -1.8330]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3, device=t.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 1, 2, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(5) # 长度为5的随机排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3, dtype=t.int) # 对角线为1, 不要求行列数一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.tensor`是在0.4版本新增加的一个新版本的创建tensor方法，使用的方法，和参数几乎和`np.array`完全一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar: tensor(3.1416), shape of sclar: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "scalar = t.tensor(3.14159) \n",
    "print('scalar: %s, shape of sclar: %s' %(scalar, scalar.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector: tensor([1, 2]), shape of vector: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "vector = t.tensor([1, 2])\n",
    "print('vector: %s, shape of vector: %s' %(vector, vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = t.Tensor(1,2) # 注意和t.tensor([1, 2])的区别\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1000, 1.2000],\n",
       "         [2.2000, 3.1000],\n",
       "         [4.9000, 5.2000]]), torch.Size([3, 2]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = t.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
    "matrix,matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1111, 0.2222, 0.3333]], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tensor([[0.11111, 0.222222, 0.3333333]],\n",
    "                     dtype=t.float64,\n",
    "                     device=t.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_tensor = t.tensor([])\n",
    "empty_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常用Tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`tensor.view`方法可以调整tensor的形状，但必须保证调整前后元素总数一致。`view`不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。在实际应用中可能经常需要添加或减少某一维度，这时候`squeeze`和`unsqueeze`两个函数就派上用场了。2. torch.squeeze() 对于tensor变量进行维度压缩，去除维数为1的的维度。例如一矩阵维度为A*1*B*C*1*D，通过squeeze()返回向量的维度为A*B*C*D。squeeze(a)，表示将a的维数位1的维度删掉，squeeze(a,N)表示，如果第N维维数为1，则压缩去掉，否则a矩阵不变\n",
    "\n",
    " 3. torch.unsqueeze() 是squeeze()的反向操作，增加一个维度，该维度维数为1，可以指定添加的维度。例如unsqueeze(a,1)表示在1这个维度进行添加\n",
    "--------------------- \n",
    "作者：zchenack \n",
    "来源：CSDN \n",
    "原文：https://blog.csdn.net/hustchenze/article/details/78989426 \n",
    "版权声明：本文为博主原创文章，转载请附上博文链接！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11],\n",
      "         [12, 13, 14],\n",
      "         [15, 16, 17]],\n",
      "\n",
      "        [[18, 19, 20],\n",
      "         [21, 22, 23],\n",
      "         [24, 25, 26]]])\n"
     ]
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "print(a)\n",
    "print(a.view(2, 3))\n",
    "c = t.arange(0, 27)\n",
    "d = c.view(3,3,3)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "b = a.view(-1, 3) # 当某一维为-1的时候，会自动计算它的大小\n",
    "print(b)\n",
    "print(b.shape)\n",
    "print(a.view(3,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n",
      "tensor([[[0, 1, 2]],\n",
      "\n",
      "        [[3, 4, 5]]])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11],\n",
      "         [12, 13, 14],\n",
      "         [15, 16, 17]],\n",
      "\n",
      "        [[18, 19, 20],\n",
      "         [21, 22, 23],\n",
      "         [24, 25, 26]]])\n",
      "tensor([[[  0,   1,   2],\n",
      "         [  3,   4,   5],\n",
      "         [  6,   7,   8]],\n",
      "\n",
      "        [[  9,  10,  11],\n",
      "         [ 12,  13,  14],\n",
      "         [ 15,  16,  17]],\n",
      "\n",
      "        [[ 18,  19,  20],\n",
      "         [ 21, 111,  23],\n",
      "         [ 24,  25,  26]]])\n",
      "tensor([[[[  0,   1,   2],\n",
      "          [  3,   4,   5],\n",
      "          [  6,   7,   8]]],\n",
      "\n",
      "\n",
      "        [[[  9,  10,  11],\n",
      "          [ 12,  13,  14],\n",
      "          [ 15,  16,  17]]],\n",
      "\n",
      "\n",
      "        [[[ 18,  19,  20],\n",
      "          [ 21, 111,  23],\n",
      "          [ 24,  25,  26]]]])\n",
      "tensor([[[[  0,   1,   2],\n",
      "          [  3,   4,   5],\n",
      "          [  6,   7,   8]]],\n",
      "\n",
      "\n",
      "        [[[  9,  10,  11],\n",
      "          [ 12, 111,  14],\n",
      "          [ 15,  16,  17]]],\n",
      "\n",
      "\n",
      "        [[[ 18,  19,  20],\n",
      "          [ 21, 111,  23],\n",
      "          [ 24,  25,  26]]]])\n"
     ]
    }
   ],
   "source": [
    "b.unsqueeze(1) # 注意形状，在第1维（下标从0开始）上增加“１” \n",
    "#等价于 b[:,None]\n",
    "print(b[:, None].shape)\n",
    "print(b[:, None])\n",
    "print(d)\n",
    "d[2][1][1] =111\n",
    "print(d)\n",
    "e = d.unsqueeze(1)\n",
    "print(e)\n",
    "e[1][0][1][1] =111\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  0,   1,   2]],\n",
      "\n",
      "         [[  3,   4,   5]],\n",
      "\n",
      "         [[  6,   7,   8]]],\n",
      "\n",
      "\n",
      "        [[[  9,  10,  11]],\n",
      "\n",
      "         [[ 12, 111,  14]],\n",
      "\n",
      "         [[ 15,  16,  17]]],\n",
      "\n",
      "\n",
      "        [[[ 18,  19,  20]],\n",
      "\n",
      "         [[ 21, 111,  23]],\n",
      "\n",
      "         [[ 24,  25,  26]]]])\n"
     ]
    }
   ],
   "source": [
    "b.unsqueeze(-2) # -2表示倒数第二个维度\n",
    "print(d.unsqueeze(-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 2],\n",
       "          [3, 4, 5]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "c.squeeze(0) # 压缩第0维的“１”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.squeeze() # 把所有维度为“1”的压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "b # a修改，b作为view之后的，也会跟着修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`resize`是另一种可用来调整`size`的方法，但与`view`不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，看一个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[                  0,                 100,                   2],\n",
       "        [                  3,                   4,                   5],\n",
       "        [8083807916600337019, 8368314977163501932, 6582899752293330533]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(3, 3) # 旧的数据依旧保存着，多出的大小会分配新空间\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引操作\n",
    "\n",
    "Tensor支持与numpy.ndarray类似的索引操作，语法上也类似，下面通过一些例子，讲解常用的索引操作。如无特殊说明，索引出来的结果与原tensor共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6928,  0.4327, -0.0519, -0.3314],\n",
       "        [ 1.2906, -1.3836,  0.9189, -1.1551],\n",
       "        [ 1.6101, -0.0331,  0.0629,  2.1010]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6928,  0.4327, -0.0519, -0.3314])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] # 第0行(下标从0开始)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6928,  1.2906,  1.6101])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 0] # 第0列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0519)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2] # 第0行第2个元素，等价于a[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3314)\n",
      "tensor(-0.3314)\n"
     ]
    }
   ],
   "source": [
    "print(a[0, -1]) # 第0行最后一个元素\n",
    "\n",
    "print(a[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6928,  0.4327, -0.0519, -0.3314],\n",
       "        [ 1.2906, -1.3836,  0.9189, -1.1551]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2] # 前两行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6928,  0.4327],\n",
       "        [ 1.2906, -1.3836]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2, 0:2] # 前两行，第0,1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6928,  0.4327]])\n",
      "tensor([-0.6928,  0.4327])\n"
     ]
    }
   ],
   "source": [
    "print(a[0:1, :2]) # 第0行，前两列 \n",
    "print(a[0, :2]) # 注意两者的区别：形状不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# None类似于np.newaxis, 为a新增了一个轴\n",
    "# 等价于a.view(1, a.shape[0], a.shape[1])\n",
    "a[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[None].shape # 等价于a[None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,None,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4, 1, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,None,:,None,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1 # 返回一个ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2906, 1.6101, 2.1010])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>1] # 等价于a.masked_select(a>1)\n",
    "# 选择结果与原tensor不共享内存空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6928,  0.4327, -0.0519, -0.3314],\n",
       "        [ 1.2906, -1.3836,  0.9189, -1.1551]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[t.LongTensor([0,1])] # 第0行和第1行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它常用的选择函数如表3-2所示。\n",
    "\n",
    "表3-2常用的选择函数\n",
    "\n",
    "函数|功能|\n",
    ":---:|:---:|\n",
    "index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列\n",
    "masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor进行选取\n",
    "non_zero(input)|非0元素的下标\n",
    "gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样\n",
    "\n",
    "\n",
    "`gather`是一个比较复杂的操作，对一个2维tensor，输出的每个元素如下：\n",
    "\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1\n",
    "```\n",
    "三维tensor的`gather`操作同理，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取对角线的元素\n",
    "index = t.LongTensor([[0,1,2,3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [12]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素\n",
    "index = t.LongTensor([[3,2,1,0]]).t()\n",
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  9,  6,  3]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素，注意与上面的不同\n",
    "index = t.LongTensor([[3,2,1,0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3],\n",
       "        [ 5,  6],\n",
       "        [10,  9],\n",
       "        [15, 12]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取两个对角线上的元素\n",
    "index = t.LongTensor([[0,1,2,3],[3,2,1,0]]).t()\n",
    "b = a.gather(1, index)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与`gather`相对应的逆操作是`scatter_`，`gather`把数据从input中按index取出，而`scatter_`是把取出的数据再放回去。注意`scatter_`函数是inplace操作。\n",
    "\n",
    "```python\n",
    "out = input.gather(dim, index)\n",
    "-->近似逆操作\n",
    "out = Tensor()\n",
    "out.scatter_(dim, index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.FloatTensor but found type torch.LongTensor for argument #4 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-212678a9ace2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 把两个对角线元素放回去到指定位置\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.FloatTensor but found type torch.LongTensor for argument #4 'src'"
     ]
    }
   ],
   "source": [
    "# 把两个对角线元素放回去到指定位置\n",
    "c = t.zeros(4,4)\n",
    "c.scatter_(1, index, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对tensor的任何索引操作仍是一个tensor，想要获取标准的python对象数值，需要调用`tensor.item()`, 这个方法只对包含一个元素的tensor适用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0] #依旧是tensor）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0].item() # python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a[0:1, 0:1, None]\n",
    "print(d.shape)\n",
    "d.item() # 只包含一个元素的tensor即可调用tensor.item,与形状无关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a[0].item()  ->\n",
    "# raise ValueError: only one element tensors can be converted to Python scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级索引\n",
    "PyTorch在0.2版本中完善了索引操作，目前已经支持绝大多数numpy的高级索引[^10]。高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的Tensor贡献内出。 \n",
    "[^10]: https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   1.,   2.],\n",
       "         [  3.,   4.,   5.],\n",
       "         [  6.,   7.,   8.]],\n",
       "\n",
       "        [[  9.,  10.,  11.],\n",
       "         [ 12.,  13.,  14.],\n",
       "         [ 15.,  16.,  17.]],\n",
       "\n",
       "        [[ 18.,  19.,  20.],\n",
       "         [ 21.,  22.,  23.],\n",
       "         [ 24.,  25.,  26.]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0,27).view(3,3,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 14.,  24.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1, 2], [1, 2], [2, 0]] # x[1,1,2]和x[2,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 19.,  10.,   1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2, 1, 0], [0], [1]] # x[2,0,1],x[1,0,1],x[0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   1.,   2.],\n",
       "         [  3.,   4.,   5.],\n",
       "         [  6.,   7.,   8.]],\n",
       "\n",
       "        [[ 18.,  19.,  20.],\n",
       "         [ 21.,  22.,  23.],\n",
       "         [ 24.,  25.,  26.]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 2], ...] # x[0] 和 x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor类型\n",
    "\n",
    "Tensor有不同的数据类型，如表3-3所示，每种类型分别对应有CPU和GPU版本(HalfTensor除外)。默认的tensor是FloatTensor，可通过`t.set_default_tensor_type` 来修改默认tensor类型(如果默认类型为GPU tensor，则所有操作都将在GPU上进行)。Tensor的类型对分析内存占用很有帮助。例如对于一个size为(1000, 1000, 1000)的FloatTensor，它有`1000*1000*1000=10^9`个元素，每个元素占32bit/8 = 4Byte内存，所以共占大约4GB内存/显存。HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大缓解GPU显存不足的问题，但由于HalfTensor所能表示的数值大小和精度有限[^2]，所以可能出现溢出等问题。\n",
    "\n",
    "[^2]: https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste\n",
    "\n",
    "表3-3: tensor数据类型\n",
    "\n",
    "| Data type                | dtype                             | CPU tensor                                                   | GPU tensor                |\n",
    "| ------------------------ | --------------------------------- | ------------------------------------------------------------ | ------------------------- |\n",
    "| 32-bit floating point    | `torch.float32` or `torch.float`  | `torch.FloatTensor`                                          | `torch.cuda.FloatTensor`  |\n",
    "| 64-bit floating point    | `torch.float64` or `torch.double` | `torch.DoubleTensor`                                         | `torch.cuda.DoubleTensor` |\n",
    "| 16-bit floating point    | `torch.float16` or `torch.half`   | `torch.HalfTensor`                                           | `torch.cuda.HalfTensor`   |\n",
    "| 8-bit integer (unsigned) | `torch.uint8`                     | [`torch.ByteTensor`](https://pytorch.org/docs/stable/tensors.html#torch.ByteTensor) | `torch.cuda.ByteTensor`   |\n",
    "| 8-bit integer (signed)   | `torch.int8`                      | `torch.CharTensor`                                           | `torch.cuda.CharTensor`   |\n",
    "| 16-bit integer (signed)  | `torch.int16` or `torch.short`    | `torch.ShortTensor`                                          | `torch.cuda.ShortTensor`  |\n",
    "| 32-bit integer (signed)  | `torch.int32` or `torch.int`      | `torch.IntTensor`                                            | `torch.cuda.IntTensor`    |\n",
    "| 64-bit integer (signed)  | `torch.int64` or `torch.long`     | `torch.LongTensor`                                           | `torch.cuda.LongTensor`   |\n",
    "\n",
    " \n",
    "\n",
    "各数据类型之间可以互相转换，`type(new_type)`是通用的做法，同时还有`float`、`long`、`half`等快捷方法。CPU tensor与GPU tensor之间的互相转换通过`tensor.cuda`和`tensor.cpu`方法实现，此外还可以使用`tensor.to(device)`。Tensor还有一个`new`方法，用法与`t.Tensor`一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。`torch.*_like(tensora)` 可以生成和`tensora`拥有同样属性(类型，形状，cpu/gpu)的新tensor。 `tensor.new_*(new_shape)` 新建一个不同形状的tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置默认tensor，注意参数是字符串\n",
    "t.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2,3)\n",
    "a.dtype # 现在a是DoubleTensor,dtype是float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 恢复之前的默认设置\n",
    "t.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把a转成FloatTensor，等价于b=a.type(t.FloatTensor)\n",
    "b = a.float() \n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.type_as(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.6390e-310,  1.4147e+161,   1.4917e+20],\n",
       "        [ 2.0093e+174,  1.4327e+228,  1.3404e-317]], dtype=torch.float64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new(2,3) # 等价于torch.DoubleTensor(2,3)，建议使用a.new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a) #等价于t.zeros(a.shape,dtype=a.dtype,device=a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a, dtype=t.int16) #可以修改某些属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8882,  0.7037,  0.5756],\n",
       "        [ 0.7113,  0.2868,  0.7597]], dtype=torch.float64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rand_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1,  1,  1],\n",
       "        [ 1,  1,  1,  1,  1],\n",
       "        [ 1,  1,  1,  1,  1],\n",
       "        [ 1,  1,  1,  1,  1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_ones(4,5, dtype=t.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  4.], dtype=torch.float64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_tensor([3,4]) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逐元素操作\n",
    "\n",
    "这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如表3-4所示。\n",
    "\n",
    "表3-4: 常见的逐元素操作\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|abs/sqrt/div/exp/fmod/log/pow..|绝对值/平方根/除法/指数/求余/求幂..|\n",
    "|cos/sin/asin/atan2/cosh..|相关三角函数|\n",
    "|ceil/round/floor/trunc| 上取整/四舍五入/下取整/只保留整数部分|\n",
    "|clamp(input, min, max)|超过min和max部分截断|\n",
    "|sigmod/tanh..|激活函数\n",
    "\n",
    "对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如`a ** 2` 等价于`torch.pow(a,2)`, `a * 2`等价于`torch.mul(a,2)`。\n",
    "\n",
    "其中`clamp(x, min, max)`的输出满足以下公式：\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "min,  & \\text{if  } x_i \\lt min \\\\\n",
    "x_i,  & \\text{if  } min \\le x_i \\le max  \\\\\n",
    "max,  & \\text{if  } x_i \\gt max\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "`clamp`常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000000000, 0.5403022766, -0.4161468446],\n",
       "        [-0.9899924994, -0.6536436081, 0.2836622000]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3)\n",
    "t.cos(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 0.,  1.,  2.]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % 3 # 等价于t.fmod(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.],\n",
       "        [  9.,  16.,  25.]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2 # 等价于t.pow(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.,  3.],\n",
       "        [ 3.,  4.,  5.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取a中的每一个元素与3相比较大的一个 (小于3的截断成3)\n",
    "print(a)\n",
    "t.clamp(a, min=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000000000, 0.8414709568, 0.9092974067],\n",
       "        [0.1411200017, -0.7568024993, -0.9589242935]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.sin_() # 效果同 a = a.sin();b=a ,但是更高效节省显存\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  归并操作 \n",
    "此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法`sum`，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如表3-5所示。\n",
    "\n",
    "表3-5: 常用归并操作\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|mean/sum/median/mode|均值/和/中位数/众数|\n",
    "|norm/dist|范数/距离|\n",
    "|std/var|标准差/方差|\n",
    "|cumsum/cumprod|累加/累乘|\n",
    "\n",
    "以上大多数函数都有一个参数**`dim`**，用来指定这些操作是在哪个维度上执行的。关于dim(对应于Numpy中的axis)的解释众说纷纭，这里提供一个简单的记忆方式：\n",
    "\n",
    "假设输入的形状是(m, n, k)\n",
    "\n",
    "- 如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)\n",
    "- 如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)\n",
    "- 如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)\n",
    "\n",
    "size中是否有\"1\"，取决于参数`keepdim`，`keepdim=True`会保留维度`1`。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如`cumsum`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  2.,  2.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "b.sum(dim = 0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  2.,  2.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keepdim=False，不保留维度\"1\"，注意形状\n",
    "b.sum(dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  3.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   3.],\n",
       "        [  3.,   7.,  12.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3)\n",
    "print(a)\n",
    "a.cumsum(dim=1) # 沿着行累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比较\n",
    "比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如表3-6所示。\n",
    "\n",
    "表3-6: 常用比较函数\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|gt/lt/ge/le/eq/ne|大于/小于/大于等于/小于等于/等于/不等|\n",
    "|topk|最大的k个数|\n",
    "|sort|排序|\n",
    "|max/min|比较两个tensor最大最小值|\n",
    "\n",
    "表中第一行的比较操作已经实现了运算符重载，因此可以使用`a>=b`、`a>b`、`a!=b`、`a==b`，其返回结果是一个`ByteTensor`，可用来选取元素。max/min这两个操作比较特殊，以max来说，它有以下三种使用情况：\n",
    "- t.max(tensor)：返回tensor中最大的一个数\n",
    "- t.max(tensor,dim)：指定维上最大的数，返回tensor和下标\n",
    "- t.max(tensor1, tensor2): 比较两个tensor相比较大的元素\n",
    "\n",
    "至于比较一个tensor和一个数，可以使用clamp函数。下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   3.,   6.],\n",
       "        [  9.,  12.,  15.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 15.,  12.,   9.],\n",
       "        [  6.,   3.,   0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.linspace(15, 0, 6).view(2, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 1,  1,  1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a>b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  9.,  12.,  15.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>b] # a中大于b的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 15.,   6.]), tensor([ 0,  0]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(b, dim=1) \n",
    "# 第一个返回值的15和6分别表示第0行和第1行最大的元素\n",
    "# 第二个返回值的0和0表示上述最大的数是该行第0个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 15.,  12.,   9.],\n",
       "        [  9.,  12.,  15.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10.,  10.,  10.],\n",
       "        [ 10.,  12.,  15.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较a和10较大的元素\n",
    "t.clamp(a, min=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数\n",
    "\n",
    "PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如表3-7所示。\n",
    "\n",
    "表3-7: 常用的线性代数函数\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|trace|对角线元素之和(矩阵的迹)|\n",
    "|diag|对角线元素|\n",
    "|triu/tril|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/badbmm..|矩阵运算\n",
    "|t|转置|\n",
    "|dot/cross|内积/外积\n",
    "|inverse|求逆矩阵\n",
    "|svd|奇异值分解\n",
    "\n",
    "具体使用说明请参见官方文档[^3]，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的`.contiguous`方法将其转为连续。\n",
    "[^3]: http://pytorch.org/docs/torch.html#blas-and-lapack-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.t()\n",
    "b.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   9.],\n",
       "        [  3.,  12.],\n",
       "        [  6.,  15.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Tensor和Numpy\n",
    "\n",
    "Tensor和Numpy数组之间具有很高的相似性，彼此之间的互操作也非常简单高效。需要注意的是，Numpy和Tensor共享内存。由于Numpy历史悠久，支持丰富的操作，所以当遇到Tensor不支持的操作时，可先转成Numpy数组，处理后再转回tensor，其转换开销很小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones([2, 3],dtype=np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a) # 也可以直接将numpy对象传入Tensor\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1.,  100.,    1.],\n",
       "        [   1.,    1.,    1.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1]=100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 100.,   1.],\n",
       "       [  1.,   1.,   1.]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy() # a, b, c三个对象共享内存\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones([2, 3])\n",
    "# 注意和上面的a的区别（dtype不是float32）\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a) # 此处进行拷贝，不共享内存\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = t.from_numpy(a) # 注意c的类型（DoubleTensor）\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "b # b与a不共享内存，所以即使a改变了，b也不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1.,  100.,    1.],\n",
       "        [   1.,    1.,    1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c # c与a共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：** 不论输入的类型是什么，t.tensor都会进行数据拷贝，不会共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = t.tensor(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 100.,   1.],\n",
       "       [  1.,   1.,   1.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0,0]=0\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播法则(broadcast)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。\n",
    "Numpy的广播法则定义如下：\n",
    "\n",
    "- 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐\n",
    "- 两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算 \n",
    "- 当输入数组的某个维度的长度为1时，计算时沿此维度复制扩充成一样的形状\n",
    "\n",
    "PyTorch当前已经支持了自动广播法则，但是笔者还是建议读者通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错：\n",
    "\n",
    "- `unsqueeze`或者`view`，或者tensor[None],：为数据某一维的形状补1，实现法则1\n",
    "- `expand`或者`expand_as`，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。\n",
    "\n",
    "注意，repeat实现与expand相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = t.ones(3, 2)\n",
    "b = t.zeros(2, 3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1.,  1.]],\n",
       "\n",
       "        [[ 1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1.,  1.]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动广播法则\n",
    "# 第一步：a是2维,b是3维，所以先在较小的a前面补1 ，\n",
    "#               即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,\n",
    "# 第二步:   a和b在第一维和第三维形状不一样，其中一个为1 ，\n",
    "#               可以利用广播法则扩展，两个形状都变成了（2，3，2）\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1.,  1.]],\n",
       "\n",
       "        [[ 1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1.,  1.]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动广播法则\n",
    "# 或者 a.view(1,3,2).expand(2,3,2)+b.expand(2,3,2)\n",
    "a[None].expand(2, 3, 2) + b.expand(2,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存\n",
    "e = a.unsqueeze(0).expand(10000000000000, 3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 内部结构\n",
    "\n",
    "tensor的数据结构如图3-1所示。tensor分为头信息区(Tensor)和存储区(Storage)，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。\n",
    "\n",
    "一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。下面看两个例子。\n",
    "\n",
    "![图3-1: Tensor的数据结构](imgs/tensor_data_structure.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(2, 3)\n",
    "b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个对象的id值可以看作它在内存中的地址\n",
    "# storage的内存地址一样，即是同一个storage\n",
    "id(b.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0.,  100.,    2.],\n",
       "        [   3.,    4.,    5.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a改变，b也随之改变，因为他们共享storage\n",
    "a[1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 100.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[2:] \n",
    "c.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93894489135160, 93894489135152)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.data_ptr(), a.data_ptr() # data_ptr返回tensor首元素的内存地址\n",
    "# 可以看出相差8，这是因为2*4=8--相差两个元素，每个元素占4个字节(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.,  100., -100.,    3.,    4.,    5.])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0] = -100 # c[0]的内存地址对应a[2]的内存地址\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6666.,   100.,  -100.],\n",
       "        [    3.,     4.,     5.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = t.Tensor(c.storage())\n",
    "d[0] = 6666\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面４个tensor共享storage\n",
    "id(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage_offset(), c.storage_offset(), d.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = b[::2, ::2] # 隔2行/列取一个元素\n",
    "id(e.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), (6, 2))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.stride(), e.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见绝大多数操作并不修改tensor的数据，而只是修改了tensor的头信息。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。\n",
    "此外有些操作会导致tensor不连续，这时需调用`tensor.contiguous`方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享storage。\n",
    "另外读者可以思考一下，之前说过的高级索引一般不共享stroage，而普通索引共享storage，这是为什么？（提示：普通索引可以通过只修改tensor的offset，stride和size，而不修改storage来实现）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 其它有关Tensor的话题\n",
    "这部分的内容不好专门划分一小节，但是笔者认为仍值得读者注意，故而将其放在这一小节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU/CPU\n",
    "tensor可以很随意的在gpu/cpu上传输。使用`tensor.cuda(device_id)`或者`tensor.cpu()`。另外一个更通用的方法是`tensor.to(device)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = t.randn(3,4, device=t.device('cuda:1'))\n",
    "    # 等价于\n",
    "    # a.t.randn(3,4).cuda(1)\n",
    "    # 但是前者更快\n",
    "    a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4362, -0.4324, -0.3985,  0.2730],\n",
       "        [-0.1356, -0.3448,  1.5713, -0.8333],\n",
       "        [-0.0089,  1.4516, -0.2062, -0.4085]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device('cpu')\n",
    "a.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**\n",
    "- 尽量使用`tensor.to(device)`, 将`device`设为一个可配置的参数，这样可以很轻松的使程序同时兼容GPU和CPU\n",
    "- 数据在GPU之中传输的速度要远快于内存(CPU)到显存(GPU), 所以尽量避免频繁的在内存和显存中传输数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 持久化\n",
    "Tensor的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的`pickle`模块，在load时还可将GPU tensor映射到CPU或其它GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = a.cuda(1) # 把a转为GPU1上的tensor,\n",
    "    t.save(a,'a.pth')\n",
    "\n",
    "    # 加载为b, 存储于GPU1上(因为保存时tensor就在GPU1上)\n",
    "    b = t.load('a.pth')\n",
    "    # 加载为c, 存储于CPU\n",
    "    c = t.load('a.pth', map_location=lambda storage, loc: storage)\n",
    "    # 加载为d, 存储于GPU0上\n",
    "    d = t.load('a.pth', map_location={'cuda:1':'cuda:0'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化计算是一种特殊的并行计算方式，相对于一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组/向量上。向量化可极大提高科学运算的效率，Python本身是一门高级语言，使用很方便，但这也意味着很多操作很低效，尤其是`for`循环。在科学计算程序中应当极力避免使用Python原生的`for循环`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def for_loop_add(x, y):\n",
    "    result = []\n",
    "    for i,j in zip(x, y):\n",
    "        result.append(i + j)\n",
    "    return t.Tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 828 µs per loop\n",
      "10 loops, best of 3: 2.5 µs per loop\n"
     ]
    }
   ],
   "source": [
    "x = t.zeros(100)\n",
    "y = t.ones(100)\n",
    "%timeit -n 10 for_loop_add(x, y)\n",
    "%timeit -n 10 x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见二者有超过几十倍的速度差距，因此在实际使用中应尽量调用内建函数(buildin-function)，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。因此在平时写代码时，就应养成向量化的思维习惯，千万避免对较大的tensor进行逐元素遍历。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还有以下几点需要注意：\n",
    "- 大多数`t.function`都有一个参数`out`，这时候产生的结果将保存在out指定tensor之中。\n",
    "- `t.set_num_threads`可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来限制PyTorch所占用的CPU数目。\n",
    "- `t.set_printoptions`可以用来设置打印tensor时的数值精度和格式。\n",
    "下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6777e+07) tensor(1.6777e+07)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(2.0000e+07), tensor(2.0000e+07))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 20000000)\n",
    "print(a[-1], a[-2]) # 32bit的IntTensor精度有限导致溢出\n",
    "b = t.LongTensor()\n",
    "t.arange(0, 20000000, out=b) # 64bit的LongTensor不会溢出\n",
    "b[-1],b[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4317,  2.1251,  1.0684],\n",
       "        [ 1.0297,  1.0137, -0.8691]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4316843748, 2.1250586510, 1.0684313774],\n",
       "        [1.0297036171, 1.0137003660, -0.8690901995]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_printoptions(precision=10)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 小试牛刀：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。首先让我们来确认线性回归的损失函数：\n",
    "$$\n",
    "loss = \\sum_i^N \\frac 1 2 ({y_i-(wx_i+b)})^2\n",
    "$$\n",
    "然后利用随机梯度下降法更新参数$\\textbf{w}$和$\\textbf{b}$来最小化损失函数，最终学得$\\textbf{w}$和$\\textbf{b}$的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "device = t.device('cpu') #如果你想用gpu，改成t.device('cuda:0'),如果用cpu 就是cpu计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 设置随机数种子，保证在不同电脑上运行时下面的输出一致\n",
    "t.manual_seed(1000) \n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    ''' 产生随机数据：y=x*2+3，加上了一些噪声'''\n",
    "    x = t.rand(batch_size, 1, device=device) * 5\n",
    "    y = x * 2 + 3 +  t.randn(batch_size, 1, device=device)\n",
    "    #x.requires_grad()\n",
    "    #x = x.float().to(device)\n",
    "    #y.requires_grad()\n",
    "    #y = x.float().to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8b34cc0eb8>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFdCAYAAACet25NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n29yK1lsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b34b79ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 来看看产生的x-y分布\n",
    "x, y = get_fake_data(batch_size=32)\n",
    "plt.scatter(x.squeeze().cpu().numpy(), y.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.LongTensor\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFdCAYAAACet25NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQMgRAwAAhBwxAABAyP1/s7bmDIoKlPAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b34b0d518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  1.9692535400390625 b:  3.1366512775421143  loss:  0.22908571362495422\n",
      "tensor([[3.1367]])\n"
     ]
    }
   ],
   "source": [
    "# 随机初始化参数\n",
    "w = t.rand(1, 1).to(device)\n",
    "b = t.zeros(1, 1).to(device)\n",
    "\n",
    "lr =0.02 # 学习率\n",
    "\n",
    "for ii in range(500):\n",
    "    x, y = get_fake_data(batch_size=4)\n",
    "    #print(str(x.type()))\n",
    "    # forward：计算loss\n",
    "    y_pred = x.mm(w) + b.expand_as(y) # x@W等价于x.mm(w);for python3 only\n",
    "    loss = 0.5 * (y_pred - y) ** 2 # 均方误差\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # backward：手动计算梯度\n",
    "    dloss = 1\n",
    "    dy_pred = dloss * (y_pred - y)\n",
    "    \n",
    "    dw = x.t().mm(dy_pred)\n",
    "    db = dy_pred.sum()\n",
    "    \n",
    "    # 更新参数\n",
    "    w.sub_(lr * dw)\n",
    "    b.sub_(lr * db)\n",
    "    \n",
    "    if ii%50 ==0:\n",
    "        # 画图\n",
    "        display.clear_output(wait=True)\n",
    "        x1 = t.arange(0, 6).view(-1, 1)\n",
    "        x1  = x1.float()\n",
    "        print(w.long().type())\n",
    "        y1 = x1.mm(w) + b.expand_as(x1)\n",
    "        plt.plot(x1.cpu().numpy(), y1.cpu().numpy()) # predicted\n",
    "        \n",
    "        x2, y2 = get_fake_data(batch_size=32) \n",
    "        plt.scatter(x2.numpy(), y2.numpy()) # true data\n",
    "        \n",
    "        #plt.xlim(0, 5)\n",
    "        #plt.ylim(0, 15)\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "print('w: ', w.item(), 'b: ', b.item(),' loss: ', loss.item())\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我直接运行报错：\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-78-a996493e38fa> in <module>()\n",
    "     29         display.clear_output(wait=True)\n",
    "     30         x = t.arange(0, 6).view(-1, 1)\n",
    "---> 31         y = x.mm(w) + b.expand_as(x)\n",
    "     32         plt.plot(x.cpu().numpy(), y.cpu().numpy()) # predicted\n",
    "     33 \n",
    "\n",
    "RuntimeError: Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #2 'mat2'\n",
    "\n",
    "\n",
    "将 x1转换类型为x1  = x1.float()\n",
    "\n",
    "\n",
    "\n",
    "下面简单介绍一下Pytorch中变量之间的相互转换。\n",
    "\n",
    "（1）CPU或GPU张量之间的转换\n",
    "\n",
    "一般只要在Tensor后加long(), int(), double(),float(),byte()等函数就能将Tensor进行类型转换；\n",
    "\n",
    "例如：Torch.LongTensor--->Torch.FloatTensor, 直接使用data.float()即可\n",
    "\n",
    "还可以使用type()函数，data为Tensor数据类型，data.type()为给出data的类型，如果使用data.type(torch.FloatTensor)则强制转换为torch.FloatTensor类型张量。\n",
    "\n",
    "当你不知道要转换为什么类型时，但需要求a1,a2两个张量的乘积，可以使用a1.type_as(a2)将a1转换为a2同类型。\n",
    "\n",
    "（2）CPU张量 ---->  GPU张量, 使用data.cuda()\n",
    "\n",
    "（3）GPU张量 ----> CPU张量 使用data.cpu()\n",
    "\n",
    "（4）Variable变量转换成普通的Tensor，其实可以理解Variable为一个Wrapper，里头的data就是Tensor. 如果Var是Variable变量，使用Var.data获得Tensor变量\n",
    "\n",
    "（5）Tensor与Numpy Array之间的转换\n",
    "\n",
    "Tensor---->Numpy  可以使用 data.numpy()，data为Tensor变量\n",
    "\n",
    "Numpy ----> Tensor 可以使用torch.from_numpy(data)，data为numpy变量\n",
    "--------------------- \n",
    "作者：zchenack \n",
    "来源：CSDN \n",
    "原文：https://blog.csdn.net/hustchenze/article/details/79154139 \n",
    "版权声明：本文为博主原创文章，转载请附上博文链接！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见程序已经基本学出w=2、b=3，并且图中直线和数据已经实现较好的拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然上面提到了许多操作，但是只要掌握了这个例子基本上就可以了，其他的知识，读者日后遇到的时候，可以再看看这部份的内容或者查找对应文档。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
